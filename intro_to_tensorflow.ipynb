{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first of all: create a virtual environment to avoid future conflict dependency problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([]), TensorShape([3]), TensorShape([2, 2]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating tensors\n",
    "scalar = tf.constant(1)  # 0D tensor (scalar)\n",
    "vector = tf.constant([1, 2, 3])  # 1D tensor (vector)\n",
    "matrix = tf.constant([[1, 2], [3, 4]])  # 2D tensor (matrix)\n",
    "\n",
    "scalar.shape, vector.shape, matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph execution vs eager execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses computional (directed acyclic) graphs that contains nodes (which are the operations, such as addition, multiplication, etc.) and edges are the data (tensors) to  carry information from one node (one operation) to another.\n",
    "\n",
    "[Example](figures/graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.75>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple computational graph\n",
    "a = tf.constant(15)\n",
    "b = tf.constant(5)\n",
    "product = tf.multiply(a, b)\n",
    "sum = tf.add(a, b)\n",
    "res = tf.divide(product, sum)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph execution:\n",
    "- The computational graph allows for a very simple way to explain the dependencies between the elements of the graph. This straightforward approach allows for using parallel execution (multiple threads, GPUs and/or machiens), e.g., we could execute the multiplication and addition in parallel as shown in the figure, since they are independent of each other.\n",
    "- portability due to the language-independent of the graph, i.e. you can write the graph in python then save it and restore it again using another language, e.g., c++.\n",
    "\n",
    "Bottomline: graph execution offers an efficient and fast way to parallelize the computations, which is a must when using big models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.75\n"
     ]
    }
   ],
   "source": [
    "# Run the computation using eager execution (no computational graph)\n",
    "result = res.numpy()\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, **eager excution** is more user friendly which allows for debugging, understanding the code and using language dependent statements, for example, the loop and if statement in python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best of both worlds: 'TF.function' decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def demonstrate():\n",
    "    a = tf.constant(15)\n",
    "    b = tf.constant(5)\n",
    "    product = tf.multiply(a, b)\n",
    "    sum = tf.add(a, b)\n",
    "    res = tf.divide(product, sum)\n",
    "    # print(\"Tracing\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.75>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = demonstrate()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager Execution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.75>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = demonstrate()\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple feedforward neural network\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, you need to compile the model by specifying the optimizer,\n",
    "# loss function, and evaluation metrics.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths with the paths to the downloaded CSV files\n",
    "train_path = \"./data/mnist_train.csv\"\n",
    "test_path = \"./data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "# Extract labels (first column) and features (remaining columns)\n",
    "train_labels = train_data.iloc[:, 0].to_numpy()\n",
    "train_images = train_data.iloc[:, 1:].to_numpy()\n",
    "test_labels = test_data.iloc[:, 0].to_numpy()\n",
    "test_images = test_data.iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59999, 28, 28, 1), (9999, 28, 28, 1))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the images to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Reshape the input data to match the expected input shape of the model\n",
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 32s 16ms/step - loss: 0.1720 - accuracy: 0.9493\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0600 - accuracy: 0.9812\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0400 - accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0273 - accuracy: 0.9915\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0201 - accuracy: 0.9937\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0143 - accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0102 - accuracy: 0.9966\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0086 - accuracy: 0.9969\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0062 - accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0066 - accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0561 - accuracy: 0.9855\n",
      "Test loss: 0.056057535111904144\n",
      "Test accuracy: 0.9854985475540161\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, label):\n",
    "    plt.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(test_images[10], test_labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to preprocess the test image (reshape to match the input format)\n",
    "image = np.expand_dims(test_images[10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.7443577e-10, 2.4801924e-16, 3.6914200e-15, 2.5473406e-17,\n",
       "        2.1365919e-16, 3.3986834e-16, 1.0000000e+00, 2.3126091e-20,\n",
       "        1.0146026e-09, 1.4652194e-20]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"data/saved_models/mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = load_model(\"data/saved_models/mnist_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- https://d3lm.medium.com/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
    "- https://www.tensorflow.org/guide/intro_to_graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
